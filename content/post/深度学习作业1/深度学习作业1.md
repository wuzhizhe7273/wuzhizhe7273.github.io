---
title: "深度学习作业1"
date: 2022-05-15T15:55:04+08:00
draft: false
categories:
    - 作业
tags:
    - 深度学习
    - 神经网络
    - 手写数字识别
image: image/浪花希儿.jpeg
slug: 深度学习作业1
description: 大三下学期深度学习课程的第一次实验
math: true
---
## 实验要求
1. 能够对样本数据进行归一化处理；
2. 能够设计深度神经网络模型；
3. 能够分析神经网络中层的表征意义；
4. 能够掌握模型的优化方法和评价指标；
5. 总结

## 报告要求
给定mnist手写数字图像数据库，自行下载训练集和测试集。要求如下：
1. 对于测试集数据，完成分类预测，实验精度达到98%以上。
2. 给出每个神经网络层的物理解释，阐述其表征意义。
3. 绘制程序流程图（要求用visio制图）
4. 源代码及必要注释
5. 总结

## 程序流程图
![流程图](post/深度学习作业1/流程图.png)

## 读取MINST数据集
这里直接使用tensorflow的api来读取，`tensorflow.keras.datasets.mnist`是一个用来读取minist数据集的模块，调用时它会自动从[https://storage.googleapis.com/tensorflow/tf-keras-datasets/minist.npz](https://storage.googleapis.com/tensorflow/tf-keras-datasets/minist.npz)下载文件,.npz文件里保存了多个numpy数组。可以用`numpy.load()`读取，tensorflow直接用`minist.load_data()`完成这个过程。
读取出来的训练集形状为(60000,28,28),即有60000张$(28 \times 28)$的图片，每一位的数据是0-255的整数，训练集标签形状为(60000,),每一位的数据是0-9的整数,为与训练集数据对应的60000个标签。测试集里数据量为10000,其它方面与训练集相同。
读取代码如下：
~~~python
minist=tf.keras.datasets.mnist
(x_train,y_train),(x_test,y_test)=minist.load_data()
~~~
训练集中前十张图片如下：
![前十张图片](post/深度学习作业1/前十张图片.png)
它们对应的标签如下：
~~~
[5 0 4 1 9 2 1 3 1 4]
~~~
## 数据处理
处理目标是将图像数据转化为网络所要求的形状，将每一位的值缩放到[0,1]的区间，并将分类标签变为one-hot编码的形式。
首先要将数据从`uint8`形式转换到`float32`形式，再将其除以255，由于numpy的广播机制，矩阵中的每一个数据都会被除以255。然后将表示图像的维度展平，代码如下：
~~~python
x_train=x_train.astype(np.float32)
x_test=x_test.astype(np.float32)
x_train=x_train.reshape(-1,28*28)/255
x_test=x_test.reshape(-1,28*28)/255
~~~
转化后训练集第一张图部分数据如下图：

![处理后第一张图数据](post/深度学习作业1/处理后第一张图数据.jpg)

接下来将标签变为one-hot形式：
只要利用`tensorflow.one_hot`就可以方便的将标签转为one_hot编码：
~~~python
y_train_onehot=tf.one_hot(y_train,10)
y_test_onehot=tf.one_hot(y_test,10)
~~~
转换后的前十个标签如下：
~~~
tf.Tensor(
[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)
~~~
## 模型构建
### 模型结构
这里利用Keras模型子类化API构建`tf.keras`模型，第一层是输入层，第二、三层是隐含层，有200个神经元，采用最后一层是输出层，有十个神经元表示十个类别，通过softmax函数将其转化为概率。  
softmax函数的形式为：
$$
softmax(x_i)=\frac{e^{x_i}}{\sum_{c=1}^{C}e^{x_c}}
$$
其中$x_i$某一个神经元输出，$C$ 表示输出神经元个数。
ReLU函数的形式为:
$$
Relu(x)=\begin{cases}
    x \quad &x>0\\\\
    0 \quad &x\leqslant 0
\end{cases}
$$
它是一个分段线性函数，把所有负值都变为0,而正值不变，这叫做单侧抑制，使得网络中的神经元具有稀疏性。
~~~python
class digit_classifier(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.flatten=tf.keras.layers.Flatten()
        self.d1=layers.Dense(units=200,activation=tf.nn.relu)
        self.tmp=layers.Dense(units=200,activation=tf.nn.relu)
        self.d2=layers.Dense(units=10)

    def call(self,inputs):
        x=self.flatten(inputs)
        x=self.d1(x)
        x=self.tmp(x)
        x=self.d2(x)
        output=tf.nn.softmax(x)
        return output
~~~
模型中的输入层是输入的原始数据，是数据处理后的特征，隐含层可以看作是对特征进行变换的过程，将原始特征映射到另一个空间，输出层输出我们想要的结果，而每层的激活函数，为神经网络提供了对特征进行非线性变换的能力。
### 模型训练
训练参数如下：
~~~python
num_epochs=5
batch_size=50
learining_rate=0.001
~~~
其中batch代表每次喂给模型的数据量，epoch代表训练轮数，一轮指训练集中所有数据都喂给模型一次，learning_rate值学习率，训练中每个参数变化的量是学习率和损失函数对于参数的梯度的乘积的复数。
选择优化器和损失函数：
优化器是指模型在计算反向传播时进行梯度下降的方式，这里使用随机梯度下降的方式来进行模型训练。
损失函数是用来评价模型精度的指标，这里采用交叉熵损失函数,其形式如下：
$$
H(P,Q)=- \sum_{i=1}^{n} P(x_i)\log Q(x_i)
$$  
$P(x_i)$表示标签的概率分布，$Q(x_i)$表示输出的概率分布,具体到本次实验$P$表示标签的one-hot向量，Q表示经softmax函数处理后神经网络输出的概率。
~~~python
loss_object=tf.keras.losses.SparseCategoricalCrossentropy()
optimizer=tf.keras.optimizers.SGD()
~~~
构建数据生成器来随机获取数据，这里为了能够保证数据和标签的对应，采取了生成随机下标再从数据集中截取的策略。
~~~python
def get_batch(data_x,data_y,batch_size):
    if(data_x.shape[0]<batch_size):
        ex=Exception("data.shape:{}小于batch_size:{}".format(data_x.shape,batch_size))
        raise(ex)
    index=np.random.randint(0,data_x.shape[0],batch_size)
    return data_x[index],data_y[index]
~~~
### 训练
模型训练的代码如下：
~~~python
#生成模型
model=digit_classifier()
# 计算batch数
num_batches=int(x_train.shape[0]//batch_size*num_epochs)
#循环训练
for batch_index in range(num_batches):
    #获取数据
    X,y=get_batch(x_train,y_train,num_batches)
    # 定义损失计算过程
    with tf.GradientTape() as tape:
        y_pred=model(X)
        loss=tf.keras.losses.sparse_categorical_crossentropy(y_true=y,y_pred=y_pred)
        loss=tf.reduce_mean(loss)
        print("batch{}:loss{}".format(batch_index,loss.numpy()))
    #计算梯度
    grads=tape.gradient(loss,model.variables)
    #更新参数
    optimizer.apply_gradients(grads_and_vars=zip(grads,model.variables))
~~~
训练中最后阶段损失变化如下：
~~~
batch5949:loss0.2160509079694748
batch5950:loss0.189644917845726
batch5951:loss0.20118406414985657
batch5952:loss0.2010451704263687
batch5953:loss0.20327620208263397
batch5954:loss0.19938337802886963
batch5955:loss0.19301575422286987
batch5956:loss0.2004004567861557
batch5957:loss0.21394090354442596
batch5958:loss0.20614023506641388
batch5959:loss0.19057206809520721
batch5960:loss0.20311588048934937
batch5961:loss0.20667105913162231
batch5962:loss0.20235677063465118
batch5963:loss0.18445037305355072
batch5964:loss0.18567191064357758
batch5965:loss0.20387212932109833
batch5966:loss0.19784651696681976
batch5967:loss0.20243585109710693
batch5968:loss0.19925585389137268
batch5969:loss0.20492398738861084
batch5970:loss0.19594500958919525
batch5971:loss0.19701890647411346
batch5972:loss0.1970183551311493
batch5973:loss0.2037982940673828
batch5974:loss0.20621439814567566
batch5975:loss0.20869891345500946
batch5976:loss0.21560095250606537
batch5977:loss0.2169310301542282
batch5978:loss0.2126057893037796
batch5979:loss0.1945841908454895
batch5980:loss0.20932133495807648
batch5981:loss0.1969047486782074
batch5982:loss0.19405750930309296
batch5983:loss0.21001195907592773
batch5984:loss0.19299526512622833
batch5985:loss0.19873084127902985
batch5986:loss0.19430774450302124
batch5987:loss0.2155349999666214
batch5988:loss0.19589634239673615
batch5989:loss0.20590727031230927
batch5990:loss0.1981458067893982
batch5991:loss0.20385988056659698
batch5992:loss0.19252967834472656
batch5993:loss0.2159004956483841
batch5994:loss0.21268168091773987
batch5995:loss0.20259033143520355
batch5996:loss0.2104693055152893
batch5997:loss0.2218792736530304
batch5998:loss0.2026265263557434
batch5999:loss0.1940533071756363
~~~
### 模型评估
要对模型评估首先要选择评价指标，这里选择`tf.keras.metrics.SparseCategoricalAccuracy()`评估其准确率
~~~python
#精度计算函数
sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()
#batch数
num_batches = int(x_test.shape[0] // batch_size)
for batch_index in range(num_batches):
    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size
    y_pred = model.predict(x_test[start_index: end_index])
    sparse_categorical_accuracy.update_state(y_true=y_test[start_index: end_index], y_pred=y_pred)
print("test accuracy: %f" % sparse_categorical_accuracy.result())
~~~
其输出如下：
~~~
test accuracy: 0.942200
~~~
## 总结
* `tensorflow.one_hot()`
  完整形式：`one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None)`
  该函数的功能主要是转换成one_hot类型的张量输出。
  参数功能如下：
  * indices中的元素指示on_value的位置，不指示的地方都为off_value。indices可以是向量、矩阵。
  * depth表示输出张量的尺寸，indices中元素默认不超过（depth-1），如果超过，输出为[0,0,···,0]
  * on_value默认为1
  * off_value默认为0
  * dtype默认为tf.float32

* `tf.GradientTape()`
  GradientTape可以理解为“梯度流 记录磁带”，在`with tf.GradientTape() as tape:`中的计算过程都会被记录下来，然后`tensorflow`会使用反向自动微分来计算相关梯度。
  计算梯度的接口为`tape.gradient()`,一般输入两个参数，计算第一个参数对第二个参数的导数。
  `optimizer.apply_gradients(grads_and_vars=zip(grads,model.variables))`
  这个参数接受梯度和参数的对的列表。
* 评价指标： `tf.keras.metrics.SparseCategoricalAccuracy(y_true, y_pred)`
  一般y_true是一个值，y_pred是一个列表，他会对比y_true于y_pred最大值的下标是否相等。
* 使用tensorflow构建模型：
  * 首先要定义模型结构：方法是继承 `tf.keras.Model`构建一个类，在`__init__`函数里定义模型的层，然后重载`cal(self,inputs)`函数，定义前向计算过程。
  * 然后选择优化器和损失函数，在`with tf.GradientTape() as tape:`记录损失计算过程，使用`tape.gradient(loss,model.variables)`计算梯度，使用`optimizer.apply_gradients(grads_and_vars=zip(grads,model.variables))`对参数进行更新。将以上步骤重复多次。
  * 模型评估：`tf.keras.metrics`下是tensorflow的评估函数模块。`xxx.update()`接受两个参数，一个是真实标签，一个是预测标签。每次向其输入数据，他都会更新当前精度。使用`xxx.result()`即可输出结果。
* 具体来说模型的层数越多，每层的神经元个数越多其训练消耗越大，预测能力越强。
* 关于SGD：SGD指随机梯度下降,SGD算法是从样本中随机抽出一组，训练后按梯度更新一次，然后再抽取一组，再更新一次，在样本量及其大的情况下，可能不用训练完所有的样本就可以获得一个损失值在可接受范围之内的模型了。
* 关于batch大小对模型的影响：
  * Batch_Size 太小，模型表现效果极其糟糕(error飙升)。
  * 随着 Batch_Size 增大，处理相同数据量的速度越快。
  * 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。
  * 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。
  * 由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。

## 源代码
必要源代码已插入正文中
 