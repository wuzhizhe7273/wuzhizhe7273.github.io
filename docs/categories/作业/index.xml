<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>作业 on Wanderer Fantansy</title>
    <link>https://wuzhizhe7273.github.io/categories/%E4%BD%9C%E4%B8%9A/</link>
    <description>Recent content in 作业 on Wanderer Fantansy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 15 May 2022 15:55:04 +0800</lastBuildDate><atom:link href="https://wuzhizhe7273.github.io/categories/%E4%BD%9C%E4%B8%9A/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>深度学习作业1</title>
      <link>https://wuzhizhe7273.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BD%9C%E4%B8%9A1/</link>
      <pubDate>Sun, 15 May 2022 15:55:04 +0800</pubDate>
      
      <guid>https://wuzhizhe7273.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BD%9C%E4%B8%9A1/</guid>
      <description>实验要求  能够对样本数据进行归一化处理； 能够设计深度神经网络模型； 能够分析神经网络中层的表征意义； 能够掌握模型的优化方法和评价指标； 总结  报告要求 给定mnist手写数字图像数据库，自行下载训练集和测试集。要求如下：
 对于测试集数据，完成分类预测，实验精度达到98%以上。 给出每个神经网络层的物理解释，阐述其表征意义。 绘制程序流程图（要求用visio制图） 源代码及必要注释 总结  程序流程图 读取MINST数据集 这里直接使用tensorflow的api来读取，tensorflow.keras.datasets.mnist是一个用来读取minist数据集的模块，调用时它会自动从https://storage.googleapis.com/tensorflow/tf-keras-datasets/minist.npz下载文件,.npz文件里保存了多个numpy数组。可以用numpy.load()读取，tensorflow直接用minist.load_data()完成这个过程。 读取出来的训练集形状为(60000,28,28),即有60000张$(28 \times 28)$的图片，每一位的数据是0-255的整数，训练集标签形状为(60000,),每一位的数据是0-9的整数,为与训练集数据对应的60000个标签。测试集里数据量为10000,其它方面与训练集相同。 读取代码如下：
minist=tf.keras.datasets.mnist (x_train,y_train),(x_test,y_test)=minist.load_data() 训练集中前十张图片如下： 它们对应的标签如下：
[5 0 4 1 9 2 1 3 1 4] 数据处理 处理目标是将图像数据转化为网络所要求的形状，将每一位的值缩放到[0,1]的区间，并将分类标签变为one-hot编码的形式。 首先要将数据从uint8形式转换到float32形式，再将其除以255，由于numpy的广播机制，矩阵中的每一个数据都会被除以255。然后将表示图像的维度展平，代码如下：
x_train=x_train.astype(np.float32) x_test=x_test.astype(np.float32) x_train=x_train.reshape(-1,28*28)/255 x_test=x_test.reshape(-1,28*28)/255 转化后训练集第一张图部分数据如下图：
接下来将标签变为one-hot形式： 只要利用tensorflow.one_hot就可以方便的将标签转为one_hot编码：
y_train_onehot=tf.one_hot(y_train,10) y_test_onehot=tf.one_hot(y_test,10) 转换后的前十个标签如下：
tf.Tensor( [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0.</description>
    </item>
    
    <item>
      <title>机器学习作业2</title>
      <link>https://wuzhizhe7273.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BD%9C%E4%B8%9A2/</link>
      <pubDate>Wed, 04 May 2022 21:37:00 +0200</pubDate>
      
      <guid>https://wuzhizhe7273.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BD%9C%E4%B8%9A2/</guid>
      <description>1.说明线性模型得原理和实现过程，涉及公式推导。 假定给定数据集$D={(x_i,y_i)}_{i=1}^m,x_i \in \mathbb{R}^N,y_i \in \mathbb{R}$,有$f(x_i)=w^Tx_i,w^T \in \mathbb{R}^m$.$f(x_i)$的值与$y_i$尽可能接近，我们称其为线性模型。$w^T$作为一个线性算子，对输入样本空间进行线性变换，将其映射到类别空间上，要求相似的样本在空间中距离更近，被分为一类。所以对于线性模型我们要求其输入样本在样本空间的分布是线性可分的。 对于线性模型来说，我们一般会有一个性能评估指标，如线性回归的均方误差，对数几率回归的对数似然，线性判别分析的广义瑞利商。这些性能评价指标就是我们模型的优化指标，我们要选择模型的参数使这些优化指标达到最优。在限制了优化指标的最优条件时，我们就可以通过数学优化的方法求得线性模型的参数。
2.解释线性判别分析的原理，算法优缺点，可加图说明。 线性判别分析( Linear Discriminant Analysis,简称LDA)是一种经典的线性学习方法，它的基本思想是将样本投影到一条直线上，使同类样本尽可能近，不同类样本之间的距离尽可能远。前者对应同类样本的协方差尽可能小，后者对应不同样本类别的投影中心的距离尽可能大。 这里先定义几个符号：假设数据集为$D{(x_i,y_i)}{i=1}^m,y_i \in {0,1},x_i \in \mathbb{R}^m,m \in \mathbb{N}^+$那么有$X_i,\mu_i,\Sigma_i$分别表示$i \in {0,1}$类样本的集合，均值向量，协方差矩阵,投影方向为$w$那么我们的目的就变为让$w^T\Sigma_0w+w^T\Sigma_1w$尽可能小，$||w^T\mu_0-w^T\mu_1 ||{2}^{2}$尽可能大。整合一下目标变为$J=\frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\Sigma_0+\Sigma_1)w}$最大化。 如果我们定义$S_w=\Sigma_0+\Sigma_1$为类内散度矩阵，$S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T$为类间散度矩阵，则可变换到$J=\frac{w^TS_bw}{w_TS_ww}$，这里的J为广义瑞利商(在第一题中提过)。 对于广义瑞利商，其优化目标为： $$ \begin{aligned} \underset{\pmb{w}}{min} \quad &amp;amp;-\pmb{w}^T\pmb{b}_b\pmb{w}\ s.t. \quad &amp;amp;\pmb{w}^T\pmb{S}_w\pmb{w}=1 \end{aligned} $$ 由拉格朗日乘子法可以求得$\pmb{S}_b\pmb{w}=\lambda\pmb{S}_w\pmb{w}$, $\pmb{S}_b\pmb{w}$的方向恒为$\pmb{\mu}_0-\pmb{\mu}_1$,则$\pmb{w}=\pmb{S}_w^{-1}(\pmb{\mu}_0-\pmb{\mu}_1)$。之后利用$\pmb{w}^T\pmb{\mu}$对特征向量进行投影即可。 优点：线性判别分析是有有监督的，能很好得反应样本间差异 缺点：局限性大，受样本种类限制，投影空间最多为n-1维。
3.以前三层神经网络为例，说明误差逆传播算法的计算原理。
假设有一个三层神经网络，如下图: 对于训练样例$(\pmb{x}k,\pmb{y}k)$,假定输出为$\pmb{\hat{y}k}=(\hat{y}1^k,\hat{y}2^k,\cdots,\hat{y}l^k)$,即$\hat{\pmb{y}}j^k=f(\beta_j-\theta_j)$,假设使用均方误差作为评估准则，$E_k=\frac{1}{2}\sum{j=1}^l(\hat{y}j^k-y_j^k)^2$。 对于误差，我们知道其在相对于各参数负梯度方向上下降最快，所以接下来我们要求误差相对于各参数的梯度，为简化计算，我们使用$w{hj},\theta_j,\upsilon{ih},\gamma_h$举例计算，其中$\theta_j,\gamma_h$分别代表输出层第$j$个神经元，隐含第$h$个神经元的阈值，也可以叫偏置。 依据链式法则： $$ \begin{aligned} &amp;amp;\Delta w{hj}=-\eta\frac{\partial E_k}{\partial w{hj}}=-\eta\frac{\partial E_k}{\partial \hat{y_k}}\frac{\partial \hat{y_k}}{\partial \beta_j}\frac{\partial \beta_j}{\partial w{hj}}\ &amp;amp;\Delta \theta_j=-\eta \frac{\partial E_k}{\partial \theta_j}=-\eta\frac{\partial E_k}{\partial \hat{y_k}}\frac{\partial \hat{y_k}}{\partial \beta_j}\frac{\partial \beta_j}{\partial \theta_j}\ &amp;amp;\Delta \upsilon{ih}=-\eta \frac{\partial E_k}{\partial b_h}\frac{b_h}{\alpha_h}\frac{\alpha_h}{\upsilon{ih}}\ &amp;amp;\Delta \gamma_h=-\eta \frac{\partial E_k}{\partial b_h}\frac{b_h}{\alpha_h}\frac{\alpha_h}{\gamma_h}\ &amp;amp;\frac{\partial E_k}{\partial b_h}= \sum_{j=1}^l\frac{\partial E_k}{\partial \beta_j}\frac{\beta_j}{\partial b_h} \end{aligned} $$ 假设$\upsilon$为任意参数，则参数更新方式为$\upsilon\leftarrow\upsilon+\Delta \upsilon$按照以上公式更新模型参数，模型达到评估要求后，即可进行预测。</description>
    </item>
    
  </channel>
</rss>
