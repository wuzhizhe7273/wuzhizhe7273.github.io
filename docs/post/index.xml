<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Wanderer Fantansy</title>
    <link>https://wuzhizhe7273.github.io/post/</link>
    <description>Recent content in Posts on Wanderer Fantansy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 16 May 2022 11:23:45 +0800</lastBuildDate><atom:link href="https://wuzhizhe7273.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Py Pathlib Note</title>
      <link>https://wuzhizhe7273.github.io/p/py-pathlib-note/</link>
      <pubDate>Mon, 16 May 2022 11:23:45 +0800</pubDate>
      
      <guid>https://wuzhizhe7273.github.io/p/py-pathlib-note/</guid>
      <description>位于Lib/pathlib.py 是面向对象的文件系统路径，与之前的os.path功能相近，从文档来看，这个库主要是用来操作文件系统路径的，与直接操作路径的字符串相比，它可以屏蔽Unix和Windows的文件路径格式差异。网上说重要的是将os和path解耦，这个我不太懂。
用法 基础使用 导入主类 from pathlib import Path 列出子目录 p=Path(&amp;#39;.&amp;#39;) [x for x in p.iterdir() if x.is_dir()] 列出目录树下指定类型文件 list(p.glob(&amp;#39;**/*.py&amp;#39;)) 查询路径属性 p.exists() p.is_dir() </description>
    </item>
    
    <item>
      <title>深度学习实验2</title>
      <link>https://wuzhizhe7273.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E9%AA%8C2/</link>
      <pubDate>Mon, 16 May 2022 10:16:03 +0800</pubDate>
      
      <guid>https://wuzhizhe7273.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E9%AA%8C2/</guid>
      <description>实验指导  读取猫狗分类数据集 从kaggle在线网站中下载猫狗分类数据集（https://www.kaggle.com/c/dogs-vs-cats/data），这个数据集包含25000张猫狗图像（每个类别都有12500张），大小为543MB（压缩后）。下载数据并解压之后，你需要创建一个新数据集，其中包含三个子集：每个类别各1000个样本的训练集、每个类别各500个样本的验证集和每个类别各500个样本的测试集。 数据预处理 首先，将数据分为“train”文件夹、“test”文件夹、“validation”文件夹；每个文件夹内分别包含以“cat”和“dog”命名的两个文件夹；train文件夹中， cat”和“dog”类别各有1000个样本；test和validation文件夹中，“cat”和“dog”类别各有500个样本。 其次，由于数据以JPEG文件的形式保存在硬盘中，在将数据输入CNN之前，应该将数据格式化为经过预处理的浮点数张量，预处理步骤大致如下：  读取图像文件； 将JPEG文件解码为RGB像素网格； 将这些像素网格转换为浮点数张量； 将像素值（0~255范围内）缩放到[0, 1]区间（正如你所知，神经网络喜欢处理较小的输入值）。 最后，需要将每张图片的大小限定为150*150像素点大小，且将数据按批次（batch）输入模型，批次大小（batch_size）自定义；另外，需要将标签离散化。   构建模型 设计模型卷积层的个数，每一层卷积核的个数，每层的激活函数以及是否含有池化层，最后一层是输出层，它是一个包含两个神经元的softmax层，将返回一个由2个概率值（总和为1）组成的数组。每个概率值表示当前彩色图像属于猫狗中某一个类别的概率。 确定编译参数 对模型进行编译，需要设置以下三个参数：  损失函数：网络如何衡量在训练数据上的性能，即网络如何朝着正确的方向前进。 优化器：基于训练数据和损失函数来更新网络的机制。 评价指标：在训练和测试过程中需要监控的指标包括准确率、精度、召回率等。   模型的训练  将训练数据输入神经网络； 网络学习将图像和标签关联在一起； 将测试数据输入已训练模型，验证预测结果与真实标签是否匹配。    实验内容 给定猫狗分类数据集，自行下载训练集和测试集。要求如下：
 对于测试集数据，完成分类预测，实验精度达到85%以上； 给出每层卷积核的物理解释，阐述其表征意义； 绘制程序流程图（要求用visio制图）； 源代码及必要注释； 总结  报告要求  写出该流程的流程图，以及各个流程快的可视化代码和源代码，以及流程运行过程。 写出实验的心得与体会。  </description>
    </item>
    
    <item>
      <title>深度学习作业1</title>
      <link>https://wuzhizhe7273.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BD%9C%E4%B8%9A1/</link>
      <pubDate>Sun, 15 May 2022 15:55:04 +0800</pubDate>
      
      <guid>https://wuzhizhe7273.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BD%9C%E4%B8%9A1/</guid>
      <description>实验要求  能够对样本数据进行归一化处理； 能够设计深度神经网络模型； 能够分析神经网络中层的表征意义； 能够掌握模型的优化方法和评价指标； 总结  报告要求 给定mnist手写数字图像数据库，自行下载训练集和测试集。要求如下：
 对于测试集数据，完成分类预测，实验精度达到98%以上。 给出每个神经网络层的物理解释，阐述其表征意义。 绘制程序流程图（要求用visio制图） 源代码及必要注释 总结  程序流程图 读取MINST数据集 这里直接使用tensorflow的api来读取，tensorflow.keras.datasets.mnist是一个用来读取minist数据集的模块，调用时它会自动从https://storage.googleapis.com/tensorflow/tf-keras-datasets/minist.npz下载文件,.npz文件里保存了多个numpy数组。可以用numpy.load()读取，tensorflow直接用minist.load_data()完成这个过程。 读取出来的训练集形状为(60000,28,28),即有60000张$(28 \times 28)$的图片，每一位的数据是0-255的整数，训练集标签形状为(60000,),每一位的数据是0-9的整数,为与训练集数据对应的60000个标签。测试集里数据量为10000,其它方面与训练集相同。 读取代码如下：
minist=tf.keras.datasets.mnist (x_train,y_train),(x_test,y_test)=minist.load_data() 训练集中前十张图片如下： 它们对应的标签如下：
[5 0 4 1 9 2 1 3 1 4] 数据处理 处理目标是将图像数据转化为网络所要求的形状，将每一位的值缩放到[0,1]的区间，并将分类标签变为one-hot编码的形式。 首先要将数据从uint8形式转换到float32形式，再将其除以255，由于numpy的广播机制，矩阵中的每一个数据都会被除以255。然后将表示图像的维度展平，代码如下：
x_train=x_train.astype(np.float32) x_test=x_test.astype(np.float32) x_train=x_train.reshape(-1,28*28)/255 x_test=x_test.reshape(-1,28*28)/255 转化后训练集第一张图部分数据如下图：
接下来将标签变为one-hot形式： 只要利用tensorflow.one_hot就可以方便的将标签转为one_hot编码：
y_train_onehot=tf.one_hot(y_train,10) y_test_onehot=tf.one_hot(y_test,10) 转换后的前十个标签如下：
tf.Tensor( [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0.</description>
    </item>
    
    <item>
      <title>Note</title>
      <link>https://wuzhizhe7273.github.io/p/note/</link>
      <pubDate>Sun, 15 May 2022 15:05:49 +0800</pubDate>
      
      <guid>https://wuzhizhe7273.github.io/p/note/</guid>
      <description>15日 关于新买来的U盘无法存储大文件这回事 碎碎念 几天前买的U盘到了，老实说现在我对这玩意其实没多大需求。今天想把wsl里的Ubuntu备份一下,毕竟这玩意配环境配了老久，在我热衷于把啥都往里面塞之后已经10个G了。打算wsl --export Ubuntu D:/export.tar之后上传到百度云，结果这垃圾玩意居然有上传大小限制，于是就想放U盘里结果提示文件过大。
问题 查过之后发现是文件系统的问题，U盘的文件系统是FAT32，这玩意单个文件不能超过4G，将其转为NTFS就好了。转换命令是convert E:/fs:ntfs,这里的E:以后用的时候应改为插入电脑后现实的U盘盘符。
资料搜集 关于FAT32和NTFS ntfs就目前而言，多用于台式机电脑、笔记本及平板电脑、移动硬盘等使用各种大中型空间容量的磁盘。 而fat32却是在U盘、内存卡等小型磁盘上使用比较多。 NTFS主要是针对传统机械硬盘而设计的，但是对于U盘或内存卡这种Flash 闪存材料却不适用。主要是NTFS分区是采用“日志式”的文件系统，这种格式会对U盘这种闪存储介质会造成较大的负担，会直接造成U盘容易损坏。 FAT16在dos及win95系统时广泛使用，不过其在容量上有很大的限制，分区无法超过2GB。 FAT32在WIN2000和XP系统中最大分区容量限制为2TB的容量。(2TB也就是2千兆) NTFS目前来说似乎没容量限制，只要硬盘空间容量有多大，那么就NTFS就可以分到多大。 FAT32在实际运行中不支持单个文件大于4GB的文件，一旦超过容量限制那么系统就会提示磁盘空间不足。 NTFS就目前来说已经突破了单个文件4GB的容量限制，现已经差不多完全替代fat32分区格式了，已在xp/win7/win8系统中广泛运用。 目前来说似乎没容量限制，只要硬盘空间容量有多大，那么就NTFS就可以分到多大。 和这个问题有关的也就需要这么多了，多的就不记了。
16日 暂时留个空 </description>
    </item>
    
    <item>
      <title>机器学习作业2</title>
      <link>https://wuzhizhe7273.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BD%9C%E4%B8%9A2/</link>
      <pubDate>Wed, 04 May 2022 21:37:00 +0200</pubDate>
      
      <guid>https://wuzhizhe7273.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BD%9C%E4%B8%9A2/</guid>
      <description>1.说明线性模型得原理和实现过程，涉及公式推导。 假定给定数据集$D={(x_i,y_i)}_{i=1}^m,x_i \in \mathbb{R}^N,y_i \in \mathbb{R}$,有$f(x_i)=w^Tx_i,w^T \in \mathbb{R}^m$.$f(x_i)$的值与$y_i$尽可能接近，我们称其为线性模型。$w^T$作为一个线性算子，对输入样本空间进行线性变换，将其映射到类别空间上，要求相似的样本在空间中距离更近，被分为一类。所以对于线性模型我们要求其输入样本在样本空间的分布是线性可分的。 对于线性模型来说，我们一般会有一个性能评估指标，如线性回归的均方误差，对数几率回归的对数似然，线性判别分析的广义瑞利商。这些性能评价指标就是我们模型的优化指标，我们要选择模型的参数使这些优化指标达到最优。在限制了优化指标的最优条件时，我们就可以通过数学优化的方法求得线性模型的参数。
2.解释线性判别分析的原理，算法优缺点，可加图说明。 线性判别分析( Linear Discriminant Analysis,简称LDA)是一种经典的线性学习方法，它的基本思想是将样本投影到一条直线上，使同类样本尽可能近，不同类样本之间的距离尽可能远。前者对应同类样本的协方差尽可能小，后者对应不同样本类别的投影中心的距离尽可能大。 这里先定义几个符号：假设数据集为$D{(x_i,y_i)}{i=1}^m,y_i \in {0,1},x_i \in \mathbb{R}^m,m \in \mathbb{N}^+$那么有$X_i,\mu_i,\Sigma_i$分别表示$i \in {0,1}$类样本的集合，均值向量，协方差矩阵,投影方向为$w$那么我们的目的就变为让$w^T\Sigma_0w+w^T\Sigma_1w$尽可能小，$||w^T\mu_0-w^T\mu_1 ||{2}^{2}$尽可能大。整合一下目标变为$J=\frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\Sigma_0+\Sigma_1)w}$最大化。 如果我们定义$S_w=\Sigma_0+\Sigma_1$为类内散度矩阵，$S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T$为类间散度矩阵，则可变换到$J=\frac{w^TS_bw}{w_TS_ww}$，这里的J为广义瑞利商(在第一题中提过)。 对于广义瑞利商，其优化目标为： $$ \begin{aligned} \underset{\pmb{w}}{min} \quad &amp;amp;-\pmb{w}^T\pmb{b}_b\pmb{w}\ s.t. \quad &amp;amp;\pmb{w}^T\pmb{S}_w\pmb{w}=1 \end{aligned} $$ 由拉格朗日乘子法可以求得$\pmb{S}_b\pmb{w}=\lambda\pmb{S}_w\pmb{w}$, $\pmb{S}_b\pmb{w}$的方向恒为$\pmb{\mu}_0-\pmb{\mu}_1$,则$\pmb{w}=\pmb{S}_w^{-1}(\pmb{\mu}_0-\pmb{\mu}_1)$。之后利用$\pmb{w}^T\pmb{\mu}$对特征向量进行投影即可。 优点：线性判别分析是有有监督的，能很好得反应样本间差异 缺点：局限性大，受样本种类限制，投影空间最多为n-1维。
3.以前三层神经网络为例，说明误差逆传播算法的计算原理。
假设有一个三层神经网络，如下图: 对于训练样例$(\pmb{x}k,\pmb{y}k)$,假定输出为$\pmb{\hat{y}k}=(\hat{y}1^k,\hat{y}2^k,\cdots,\hat{y}l^k)$,即$\hat{\pmb{y}}j^k=f(\beta_j-\theta_j)$,假设使用均方误差作为评估准则，$E_k=\frac{1}{2}\sum{j=1}^l(\hat{y}j^k-y_j^k)^2$。 对于误差，我们知道其在相对于各参数负梯度方向上下降最快，所以接下来我们要求误差相对于各参数的梯度，为简化计算，我们使用$w{hj},\theta_j,\upsilon{ih},\gamma_h$举例计算，其中$\theta_j,\gamma_h$分别代表输出层第$j$个神经元，隐含第$h$个神经元的阈值，也可以叫偏置。 依据链式法则： $$ \begin{aligned} &amp;amp;\Delta w{hj}=-\eta\frac{\partial E_k}{\partial w{hj}}=-\eta\frac{\partial E_k}{\partial \hat{y_k}}\frac{\partial \hat{y_k}}{\partial \beta_j}\frac{\partial \beta_j}{\partial w{hj}}\ &amp;amp;\Delta \theta_j=-\eta \frac{\partial E_k}{\partial \theta_j}=-\eta\frac{\partial E_k}{\partial \hat{y_k}}\frac{\partial \hat{y_k}}{\partial \beta_j}\frac{\partial \beta_j}{\partial \theta_j}\ &amp;amp;\Delta \upsilon{ih}=-\eta \frac{\partial E_k}{\partial b_h}\frac{b_h}{\alpha_h}\frac{\alpha_h}{\upsilon{ih}}\ &amp;amp;\Delta \gamma_h=-\eta \frac{\partial E_k}{\partial b_h}\frac{b_h}{\alpha_h}\frac{\alpha_h}{\gamma_h}\ &amp;amp;\frac{\partial E_k}{\partial b_h}= \sum_{j=1}^l\frac{\partial E_k}{\partial \beta_j}\frac{\beta_j}{\partial b_h} \end{aligned} $$ 假设$\upsilon$为任意参数，则参数更新方式为$\upsilon\leftarrow\upsilon+\Delta \upsilon$按照以上公式更新模型参数，模型达到评估要求后，即可进行预测。</description>
    </item>
    
    <item>
      <title>测试文章1</title>
      <link>https://wuzhizhe7273.github.io/p/test1/</link>
      <pubDate>Mon, 10 Aug 2020 01:00:00 +0200</pubDate>
      
      <guid>https://wuzhizhe7273.github.io/p/test1/</guid>
      <description>这是测试文章1
一级标题1 怎么没反应
二级标题1 二级标题2 一级标题2 </description>
    </item>
    
    <item>
      <title>测试文章2</title>
      <link>https://wuzhizhe7273.github.io/p/test2/</link>
      <pubDate>Mon, 10 Aug 2020 01:00:00 +0200</pubDate>
      
      <guid>https://wuzhizhe7273.github.io/p/test2/</guid>
      <description>这是测试文章2</description>
    </item>
    
    <item>
      <title>测试文章3</title>
      <link>https://wuzhizhe7273.github.io/p/test3/</link>
      <pubDate>Mon, 10 Aug 2020 01:00:00 +0200</pubDate>
      
      <guid>https://wuzhizhe7273.github.io/p/test3/</guid>
      <description>这是测试文章3</description>
    </item>
    
  </channel>
</rss>
