[{"content":"实验要求  能够对样本数据进行归一化处理； 能够设计深度神经网络模型； 能够分析神经网络中层的表征意义； 能够掌握模型的优化方法和评价指标； 总结  报告要求 给定mnist手写数字图像数据库，自行下载训练集和测试集。要求如下：\n 对于测试集数据，完成分类预测，实验精度达到98%以上。 给出每个神经网络层的物理解释，阐述其表征意义。 绘制程序流程图（要求用visio制图） 源代码及必要注释 总结  程序流程图 读取MINST数据集 这里直接使用tensorflow的api来读取，tensorflow.keras.datasets.mnist是一个用来读取minist数据集的模块，调用时它会自动从https://storage.googleapis.com/tensorflow/tf-keras-datasets/minist.npz下载文件,.npz文件里保存了多个numpy数组。可以用numpy.load()读取，tensorflow直接用minist.load_data()完成这个过程。 读取出来的训练集形状为(60000,28,28),即有60000张$(28 \\times 28)$的图片，每一位的数据是0-255的整数，训练集标签形状为(60000,),每一位的数据是0-9的整数,为与训练集数据对应的60000个标签。测试集里数据量为10000,其它方面与训练集相同。 读取代码如下：\nminist=tf.keras.datasets.mnist (x_train,y_train),(x_test,y_test)=minist.load_data() 训练集中前十张图片如下： 它们对应的标签如下：\n[5 0 4 1 9 2 1 3 1 4] 数据处理 处理目标是将图像数据转化为网络所要求的形状，将每一位的值缩放到[0,1]的区间，并将分类标签变为one-hot编码的形式。 首先要将数据从uint8形式转换到float32形式，再将其除以255，由于numpy的广播机制，矩阵中的每一个数据都会被除以255。然后将表示图像的维度展平，代码如下：\nx_train=x_train.astype(np.float32) x_test=x_test.astype(np.float32) x_train=x_train.reshape(-1,28*28)/255 x_test=x_test.reshape(-1,28*28)/255 转化后训练集第一张图部分数据如下图：\n接下来将标签变为one-hot形式： 只要利用tensorflow.one_hot就可以方便的将标签转为one_hot编码：\ny_train_onehot=tf.one_hot(y_train,10) y_test_onehot=tf.one_hot(y_test,10) 转换后的前十个标签如下：\ntf.Tensor( [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32) 模型构建 模型结构 这里利用Keras模型子类化API构建tf.keras模型，第一层是输入层，第二、三层是隐含层，有200个神经元，采用最后一层是输出层，有十个神经元表示十个类别，通过softmax函数将其转化为概率。\nsoftmax函数的形式为： $$ softmax(x_i)=\\frac{e^{x_i}}{\\sum_{c=1}^{C}e^{x_c}} $$ 其中$x_i$某一个神经元输出，$C$ 表示输出神经元个数。 ReLU函数的形式为: $$ Relu(x)=\\begin{cases} x \\quad \u0026amp;x\u0026gt;0\\\\ 0 \\quad \u0026amp;x\\leqslant 0 \\end{cases} $$ 它是一个分段线性函数，把所有负值都变为0,而正值不变，这叫做单侧抑制，使得网络中的神经元具有稀疏性。\nclass digit_classifier(tf.keras.Model): def __init__(self): super().__init__() self.flatten=tf.keras.layers.Flatten() self.d1=layers.Dense(units=200,activation=tf.nn.relu) self.tmp=layers.Dense(units=200,activation=tf.nn.relu) self.d2=layers.Dense(units=10) def call(self,inputs): x=self.flatten(inputs) x=self.d1(x) x=self.tmp(x) x=self.d2(x) output=tf.nn.softmax(x) return output 模型中的输入层是输入的原始数据，是数据处理后的特征，隐含层可以看作是对特征进行变换的过程，将原始特征映射到另一个空间，输出层输出我们想要的结果，而每层的激活函数，为神经网络提供了对特征进行非线性变换的能力。\n模型训练 训练参数如下：\nnum_epochs=5 batch_size=50 learining_rate=0.001 其中batch代表每次喂给模型的数据量，epoch代表训练轮数，一轮指训练集中所有数据都喂给模型一次，learning_rate值学习率，训练中每个参数变化的量是学习率和损失函数对于参数的梯度的乘积的复数。 选择优化器和损失函数： 优化器是指模型在计算反向传播时进行梯度下降的方式，这里使用随机梯度下降的方式来进行模型训练。 损失函数是用来评价模型精度的指标，这里采用交叉熵损失函数,其形式如下： $$ H(P,Q)=- \\sum_{i=1}^{n} P(x_i)\\log Q(x_i) $$\n$P(x_i)$表示标签的概率分布，$Q(x_i)$表示输出的概率分布,具体到本次实验$P$表示标签的one-hot向量，Q表示经softmax函数处理后神经网络输出的概率。\nloss_object=tf.keras.losses.SparseCategoricalCrossentropy() optimizer=tf.keras.optimizers.SGD() 构建数据生成器来随机获取数据，这里为了能够保证数据和标签的对应，采取了生成随机下标再从数据集中截取的策略。\ndef get_batch(data_x,data_y,batch_size): if(data_x.shape[0]\u0026lt;batch_size): ex=Exception(\u0026#34;data.shape:{}小于batch_size:{}\u0026#34;.format(data_x.shape,batch_size)) raise(ex) index=np.random.randint(0,data_x.shape[0],batch_size) return data_x[index],data_y[index] 训练 模型训练的代码如下：\n#生成模型 model=digit_classifier() # 计算batch数 num_batches=int(x_train.shape[0]//batch_size*num_epochs) #循环训练 for batch_index in range(num_batches): #获取数据 X,y=get_batch(x_train,y_train,num_batches) # 定义损失计算过程 with tf.GradientTape() as tape: y_pred=model(X) loss=tf.keras.losses.sparse_categorical_crossentropy(y_true=y,y_pred=y_pred) loss=tf.reduce_mean(loss) print(\u0026#34;batch{}:loss{}\u0026#34;.format(batch_index,loss.numpy())) #计算梯度 grads=tape.gradient(loss,model.variables) #更新参数 optimizer.apply_gradients(grads_and_vars=zip(grads,model.variables)) 训练中最后阶段损失变化如下：\nbatch5949:loss0.2160509079694748 batch5950:loss0.189644917845726 batch5951:loss0.20118406414985657 batch5952:loss0.2010451704263687 batch5953:loss0.20327620208263397 batch5954:loss0.19938337802886963 batch5955:loss0.19301575422286987 batch5956:loss0.2004004567861557 batch5957:loss0.21394090354442596 batch5958:loss0.20614023506641388 batch5959:loss0.19057206809520721 batch5960:loss0.20311588048934937 batch5961:loss0.20667105913162231 batch5962:loss0.20235677063465118 batch5963:loss0.18445037305355072 batch5964:loss0.18567191064357758 batch5965:loss0.20387212932109833 batch5966:loss0.19784651696681976 batch5967:loss0.20243585109710693 batch5968:loss0.19925585389137268 batch5969:loss0.20492398738861084 batch5970:loss0.19594500958919525 batch5971:loss0.19701890647411346 batch5972:loss0.1970183551311493 batch5973:loss0.2037982940673828 batch5974:loss0.20621439814567566 batch5975:loss0.20869891345500946 batch5976:loss0.21560095250606537 batch5977:loss0.2169310301542282 batch5978:loss0.2126057893037796 batch5979:loss0.1945841908454895 batch5980:loss0.20932133495807648 batch5981:loss0.1969047486782074 batch5982:loss0.19405750930309296 batch5983:loss0.21001195907592773 batch5984:loss0.19299526512622833 batch5985:loss0.19873084127902985 batch5986:loss0.19430774450302124 batch5987:loss0.2155349999666214 batch5988:loss0.19589634239673615 batch5989:loss0.20590727031230927 batch5990:loss0.1981458067893982 batch5991:loss0.20385988056659698 batch5992:loss0.19252967834472656 batch5993:loss0.2159004956483841 batch5994:loss0.21268168091773987 batch5995:loss0.20259033143520355 batch5996:loss0.2104693055152893 batch5997:loss0.2218792736530304 batch5998:loss0.2026265263557434 batch5999:loss0.1940533071756363 模型评估 要对模型评估首先要选择评价指标，这里选择tf.keras.metrics.SparseCategoricalAccuracy()评估其准确率\n#精度计算函数 sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy() #batch数 num_batches = int(x_test.shape[0] // batch_size) for batch_index in range(num_batches): start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size y_pred = model.predict(x_test[start_index: end_index]) sparse_categorical_accuracy.update_state(y_true=y_test[start_index: end_index], y_pred=y_pred) print(\u0026#34;test accuracy: %f\u0026#34; % sparse_categorical_accuracy.result()) 其输出如下：\ntest accuracy: 0.942200 总结   tensorflow.one_hot() 完整形式：one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None) 该函数的功能主要是转换成one_hot类型的张量输出。 参数功能如下：\n indices中的元素指示on_value的位置，不指示的地方都为off_value。indices可以是向量、矩阵。 depth表示输出张量的尺寸，indices中元素默认不超过（depth-1），如果超过，输出为[0,0,···,0] on_value默认为1 off_value默认为0 dtype默认为tf.float32    tf.GradientTape() GradientTape可以理解为“梯度流 记录磁带”，在with tf.GradientTape() as tape:中的计算过程都会被记录下来，然后tensorflow会使用反向自动微分来计算相关梯度。 计算梯度的接口为tape.gradient(),一般输入两个参数，计算第一个参数对第二个参数的导数。 optimizer.apply_gradients(grads_and_vars=zip(grads,model.variables)) 这个参数接受梯度和参数的对的列表。\n  评价指标： tf.keras.metrics.SparseCategoricalAccuracy(y_true, y_pred) 一般y_true是一个值，y_pred是一个列表，他会对比y_true于y_pred最大值的下标是否相等。\n  使用tensorflow构建模型：\n 首先要定义模型结构：方法是继承 tf.keras.Model构建一个类，在__init__函数里定义模型的层，然后重载cal(self,inputs)函数，定义前向计算过程。 然后选择优化器和损失函数，在with tf.GradientTape() as tape:记录损失计算过程，使用tape.gradient(loss,model.variables)计算梯度，使用optimizer.apply_gradients(grads_and_vars=zip(grads,model.variables))对参数进行更新。将以上步骤重复多次。 模型评估：tf.keras.metrics下是tensorflow的评估函数模块。xxx.update()接受两个参数，一个是真实标签，一个是预测标签。每次向其输入数据，他都会更新当前精度。使用xxx.result()即可输出结果。    具体来说模型的层数越多，每层的神经元个数越多其训练消耗越大，预测能力越强。\n  关于SGD：SGD指随机梯度下降,SGD算法是从样本中随机抽出一组，训练后按梯度更新一次，然后再抽取一组，再更新一次，在样本量及其大的情况下，可能不用训练完所有的样本就可以获得一个损失值在可接受范围之内的模型了。\n  关于batch大小对模型的影响：\n Batch_Size 太小，模型表现效果极其糟糕(error飙升)。 随着 Batch_Size 增大，处理相同数据量的速度越快。 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。 由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。    源代码 必要源代码已插入正文中\n","date":"2022-05-15T15:55:04+08:00","image":"https://wuzhizhe7273.github.io/image/%E6%B5%AA%E8%8A%B1%E5%B8%8C%E5%84%BF.jpeg","permalink":"https://wuzhizhe7273.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BD%9C%E4%B8%9A1/","title":"深度学习作业1"},{"content":"15日 关于新买来的U盘无法存储大文件这回事 碎碎念 几天前买的U盘到了，老实说现在我对这玩意其实没多大需求。今天想把wsl里的Ubuntu备份一下,毕竟这玩意配环境配了老久，在我热衷于把啥都往里面塞之后已经10个G了。打算wsl --export Ubuntu D:/export.tar之后上传到百度云，结果这垃圾玩意居然有上传大小限制，于是就想放U盘里结果提示文件过大。\n问题 查过之后发现是文件系统的问题，U盘的文件系统是FAT32，这玩意单个文件不能超过4G，将其转为NTFS就好了。转换命令是convert E:/fs:ntfs,这里的E:以后用的时候应改为插入电脑后现实的U盘盘符。\n资料搜集 关于FAT32和NTFS ntfs就目前而言，多用于台式机电脑、笔记本及平板电脑、移动硬盘等使用各种大中型空间容量的磁盘。 而fat32却是在U盘、内存卡等小型磁盘上使用比较多。 NTFS主要是针对传统机械硬盘而设计的，但是对于U盘或内存卡这种Flash 闪存材料却不适用。主要是NTFS分区是采用“日志式”的文件系统，这种格式会对U盘这种闪存储介质会造成较大的负担，会直接造成U盘容易损坏。 FAT16在dos及win95系统时广泛使用，不过其在容量上有很大的限制，分区无法超过2GB。 FAT32在WIN2000和XP系统中最大分区容量限制为2TB的容量。(2TB也就是2千兆) NTFS目前来说似乎没容量限制，只要硬盘空间容量有多大，那么就NTFS就可以分到多大。 FAT32在实际运行中不支持单个文件大于4GB的文件，一旦超过容量限制那么系统就会提示磁盘空间不足。 NTFS就目前来说已经突破了单个文件4GB的容量限制，现已经差不多完全替代fat32分区格式了，已在xp/win7/win8系统中广泛运用。 目前来说似乎没容量限制，只要硬盘空间容量有多大，那么就NTFS就可以分到多大。 和这个问题有关的也就需要这么多了，多的就不记了。\n16日 暂时留个空 ","date":"2022-05-15T15:05:49+08:00","image":"https://wuzhizhe7273.github.io/image/%E6%B5%AA%E8%8A%B1%E5%B8%8C%E5%84%BF.jpeg","permalink":"https://wuzhizhe7273.github.io/p/note/","title":"Note"},{"content":"1.说明线性模型得原理和实现过程，涉及公式推导。 假定给定数据集$D={(x_i,y_i)}_{i=1}^m,x_i \\in \\mathbb{R}^N,y_i \\in \\mathbb{R}$,有$f(x_i)=w^Tx_i,w^T \\in \\mathbb{R}^m$.$f(x_i)$的值与$y_i$尽可能接近，我们称其为线性模型。$w^T$作为一个线性算子，对输入样本空间进行线性变换，将其映射到类别空间上，要求相似的样本在空间中距离更近，被分为一类。所以对于线性模型我们要求其输入样本在样本空间的分布是线性可分的。 对于线性模型来说，我们一般会有一个性能评估指标，如线性回归的均方误差，对数几率回归的对数似然，线性判别分析的广义瑞利商。这些性能评价指标就是我们模型的优化指标，我们要选择模型的参数使这些优化指标达到最优。在限制了优化指标的最优条件时，我们就可以通过数学优化的方法求得线性模型的参数。\n2.解释线性判别分析的原理，算法优缺点，可加图说明。 线性判别分析( Linear Discriminant Analysis,简称LDA)是一种经典的线性学习方法，它的基本思想是将样本投影到一条直线上，使同类样本尽可能近，不同类样本之间的距离尽可能远。前者对应同类样本的协方差尽可能小，后者对应不同样本类别的投影中心的距离尽可能大。 这里先定义几个符号：假设数据集为$D{(x_i,y_i)}{i=1}^m,y_i \\in {0,1},x_i \\in \\mathbb{R}^m,m \\in \\mathbb{N}^+$那么有$X_i,\\mu_i,\\Sigma_i$分别表示$i \\in {0,1}$类样本的集合，均值向量，协方差矩阵,投影方向为$w$那么我们的目的就变为让$w^T\\Sigma_0w+w^T\\Sigma_1w$尽可能小，$||w^T\\mu_0-w^T\\mu_1 ||{2}^{2}$尽可能大。整合一下目标变为$J=\\frac{w^T(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^Tw}{w^T(\\Sigma_0+\\Sigma_1)w}$最大化。 如果我们定义$S_w=\\Sigma_0+\\Sigma_1$为类内散度矩阵，$S_b=(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T$为类间散度矩阵，则可变换到$J=\\frac{w^TS_bw}{w_TS_ww}$，这里的J为广义瑞利商(在第一题中提过)。 对于广义瑞利商，其优化目标为： $$ \\begin{aligned} \\underset{\\pmb{w}}{min} \\quad \u0026amp;-\\pmb{w}^T\\pmb{b}_b\\pmb{w}\\ s.t. \\quad \u0026amp;\\pmb{w}^T\\pmb{S}_w\\pmb{w}=1 \\end{aligned} $$ 由拉格朗日乘子法可以求得$\\pmb{S}_b\\pmb{w}=\\lambda\\pmb{S}_w\\pmb{w}$, $\\pmb{S}_b\\pmb{w}$的方向恒为$\\pmb{\\mu}_0-\\pmb{\\mu}_1$,则$\\pmb{w}=\\pmb{S}_w^{-1}(\\pmb{\\mu}_0-\\pmb{\\mu}_1)$。之后利用$\\pmb{w}^T\\pmb{\\mu}$对特征向量进行投影即可。 优点：线性判别分析是有有监督的，能很好得反应样本间差异 缺点：局限性大，受样本种类限制，投影空间最多为n-1维。\n3.以前三层神经网络为例，说明误差逆传播算法的计算原理。\n假设有一个三层神经网络，如下图: 对于训练样例$(\\pmb{x}k,\\pmb{y}k)$,假定输出为$\\pmb{\\hat{y}k}=(\\hat{y}1^k,\\hat{y}2^k,\\cdots,\\hat{y}l^k)$,即$\\hat{\\pmb{y}}j^k=f(\\beta_j-\\theta_j)$,假设使用均方误差作为评估准则，$E_k=\\frac{1}{2}\\sum{j=1}^l(\\hat{y}j^k-y_j^k)^2$。 对于误差，我们知道其在相对于各参数负梯度方向上下降最快，所以接下来我们要求误差相对于各参数的梯度，为简化计算，我们使用$w{hj},\\theta_j,\\upsilon{ih},\\gamma_h$举例计算，其中$\\theta_j,\\gamma_h$分别代表输出层第$j$个神经元，隐含第$h$个神经元的阈值，也可以叫偏置。 依据链式法则： $$ \\begin{aligned} \u0026amp;\\Delta w{hj}=-\\eta\\frac{\\partial E_k}{\\partial w{hj}}=-\\eta\\frac{\\partial E_k}{\\partial \\hat{y_k}}\\frac{\\partial \\hat{y_k}}{\\partial \\beta_j}\\frac{\\partial \\beta_j}{\\partial w{hj}}\\ \u0026amp;\\Delta \\theta_j=-\\eta \\frac{\\partial E_k}{\\partial \\theta_j}=-\\eta\\frac{\\partial E_k}{\\partial \\hat{y_k}}\\frac{\\partial \\hat{y_k}}{\\partial \\beta_j}\\frac{\\partial \\beta_j}{\\partial \\theta_j}\\ \u0026amp;\\Delta \\upsilon{ih}=-\\eta \\frac{\\partial E_k}{\\partial b_h}\\frac{b_h}{\\alpha_h}\\frac{\\alpha_h}{\\upsilon{ih}}\\ \u0026amp;\\Delta \\gamma_h=-\\eta \\frac{\\partial E_k}{\\partial b_h}\\frac{b_h}{\\alpha_h}\\frac{\\alpha_h}{\\gamma_h}\\ \u0026amp;\\frac{\\partial E_k}{\\partial b_h}= \\sum_{j=1}^l\\frac{\\partial E_k}{\\partial \\beta_j}\\frac{\\beta_j}{\\partial b_h} \\end{aligned} $$ 假设$\\upsilon$为任意参数，则参数更新方式为$\\upsilon\\leftarrow\\upsilon+\\Delta \\upsilon$按照以上公式更新模型参数，模型达到评估要求后，即可进行预测。\n4.解释支持向量机的原理和实现过程，优缺点，适用情况，不同参数的作用，说明自己的学习感悟 从集合角度看，支持向量机是找一个对于线性可分的数据集正负样本都远的超平面。 我们用$\\pmb{w}^T\\pmb{x}+b=0$,其中$\\pmb{w}=(w_1;w_2;\u0026hellip;;w_d)$为超平面法向量，决定了超平面的方向，b为位移项，决定了超平面和原点之间的距离。 样本空间中任意一点到超平面的距离表示为： $$ \\gamma=\\frac{|\\pmb{w}^T\\pmb{x}+b|}{||\\pmb{w}||} $$ 使用$ \\frac{y_{min}(\\pmb{w}^T\\pmb{x}{min}+b)}{||\\pmb{x}||} $表示数据集划分的几何距离。 那么解优化问题： $$ \\begin{aligned} \\max{\\pmb{w},b}\\quad\u0026amp;\\frac{y_{min}(\\pmb{w}^T\\pmb{x}{min}+b)}{||\\pmb{w}||}\\ s.t.\\quad\u0026amp;y_i(\\pmb{w}^T\\pmb{x}i+b)\\geqslant y{min}(\\pmb{w}^T\\pmb{x}{min}+b) \\end{aligned} $$ 为使其解唯一，限制$\\frac{y_{min}(\\pmb{w}^T\\pmb{x}{min}+b)}{||\\pmb{x}||}=1$，得： $$ \\begin{aligned} \\max{\\pmb{w},b}\\quad\u0026amp;\\frac{1}{||\\pmb{w}||}\\ s.t.\\quad\u0026amp;y_i(\\pmb{w}^T\\pmb{x}_i+b)\\geqslant 1 \\end{aligned} $$ 解这个优化问题得$\\pmb{w}$和$b$,确定超平面。（真不会解，解法空着了） 使用： $$ y=sign(\\pmb{w}^T+b) $$ 即可对样本进行分类。 优点：支持向量机是一种小样本学习方法，只有少数几个样本起作用，避免了维数灾难，鲁棒性好。 缺点：对于分线性可分得数据集需要借助核函数，核函数的选择是一个未解决的问题。对于大规模训练耗时大，解决多分类问题存在困难。\n5.在夏季，某公园男性穿凉鞋的概率为$\\frac{2}{5}$,女性穿凉鞋的概率为$\\frac{4}{5}$，并且该公园中男女比例通常为2:1,问题：若你在公园中随机遇到一个穿凉鞋的人，请问他的性别为男性或女性的概率分别为多少，解释计算原理。\n贝叶斯公式： $$ P(A_i|B)=\\frac{P(B|A_i)P(A_i)}{\\sum_j P(B|A_j)P(Aj)} $$ 假设事件A表示遇到的人是男或女,事件B表示遇到的人穿凉鞋。 则依以上公式有： $$ \\begin{aligned} P(男|穿)\u0026amp;=\\frac{P(穿|男)P(男)}{P(穿|男)P(男)+P(穿|女)P(女)}\\ \u0026amp;=\\frac{\\frac{2}{5}\\times \\frac{2}{3}}{\\frac{2}{5}\\times\\frac{2}{3}+\\frac{4}{5}\\times\\frac{1}{3}}=\\frac{1}{2}=0.5\\ P(女|穿)\u0026amp;=1-P(男|穿)=0.5 \\end{aligned} $$ 所以在如题所述的公园遇到一个穿凉鞋的人，他(她)是男(女)性的概率为0.5。\n6.说明集成学习算法的主要类别，以Adaboost和随机森林为例，说明算法原理和实现过程\n集成学习通过构建多个学习器并将其结合来完成学习任务的方法。可以根据结合策略可分为串行序列化方法和并行结合方法，前者以Bossting方法为代表，个体学习器之间常常存在强依赖关系，后者则以Bagging和随机森林为代表，个体学习器之间不存在强依赖。 关于Adaboost: 此算法先从初始训练集训练出一个基学习器，在根据这个基学习器的预测结果来调整样本分布，是之前错分的样本在后续学习器的训练样本在后续受到更多关注，然后训练下一个学习器，如此重复，直至学习器数目达到事先指定的$T$,最后将T个学习器进行加权结合。 集成学习器表示为：$H(x)=\\sum_{t=1}^T\\alpha_t h_t(x)$,其中$h_t$为个体学习器，$\\alpha_t$为其对应的权重。 学习器需要最小化损失函数$\\mathcal{L}(H|\\mathcal{D})=\\mathbb{E}{x\\sim \\mathcal{D}}[e^{-f(x)H(x)}]$ 前向分布求解：每轮求解一个学习器的$h_t$和权重$\\alpha_t$,第t轮优化目标 $$ (\\alpha_t,h_t)=argmin{\\alpha,h}\\mathcal{L}(H_{t-1}+\\alpha h|\\mathcal{D}) $$ 根据指数损失函数的定义式，有 $$ \\begin{aligned} \\mathcal{L}(H_{t-1}+\\alpha h|\\mathcal{D})\u0026amp;=\\mathbb{E}{x \\sim \\mathcal{D}}[e^{-f(x)(H{t-1}(x)+\\alpha h(x))}]\\ \u0026amp;=\\sum_{i=1}^{|\\mathcal{D}|}\\mathcal{D}(x_i)e^{-f(x_i)(H_{t-1}(x_i)+\\alpha h(x_i))}\\ \u0026amp;=\\sum_{i=1}^{|\\mathcal{D}|}\\mathcal{D}(x_i)e^{-f(x_i)H_{t-1}(x_i)}e^{-f(x_i)\\alpha h(x_i)} \\end{aligned} $$ 因为$f(x_i)$和$h(x_i)$仅可取值{-1,1}，可以推得 $$ \\begin{aligned} \\mathcal{L}(H_{t-1}+\\alpha h|\\mathcal{D})\u0026amp;=\\sum_{i=1}^{|D|}\\mathcal{D}(x_i)e^{-f(x_i)H_{t-1}(x_i)}(e^{-\\alpha}+(e^{\\alpha}-e^{-\\alpha})\\mathbb{I}(f(x_i)\\neq h(x_i)))\\ \u0026amp;=\\sum_{i=1}^{|D|}\\mathcal{D}(x_i)e^{-f(x_i)H_{t-1}(x_i)}e^{-\\alpha}+\\sum_{i=1}^{|D|}\\mathcal{D}(x_i)e^{-f(x_i)H_{t-1}(x_i)}(e^{\\alpha}-e^{-\\alpha})\\mathbb{I}(f(x_i)\\neq h(x_i)) \\end{aligned} $$ 做一个简单的符号替换，令$\\mathcal{D}t^{\u0026rsquo;}(x_i)=\\mathcal{D}(x_i)e^{-f(x_i)H{t-1}(x_i)}$,化简得 $$ \\mathcal(L)(H_{t-1}+\\alpha h|\\mathcal{D})=e^{-\\alpha}\\sum_{i=1}^{|D|}\\mathcal{D}t^{\u0026rsquo;}(x_i)+(e^{\\alpha}-e^{- \\alpha})\\sum{i=1}^{|D}\\mathcal{D}t^{\u0026rsquo;}(x_i)\\mathbb{I}(f(x_i)\\neq h(x_i)) $$ 忽略和$h_t$无关的项 $$ h_t=argmin_h(e^{\\alpha}-e^{-\\alpha})\\sum{i=1}^{|D|}\\mathcal{D_t{\u0026rsquo;}(x_i)}\\mathbb{I}(f(x_i)\\neq h(x_i)) $$ 由于$\\alpha\u0026gt;\\frac{1}{2}$,所以$e^{\\alpha}-e^{-\\alpha}\u0026gt;0$恒成立。 求解目标简化为 $$ h_t=argmin_h\\sum_{i=1}^{|D}\\mathcal{D}t^{\u0026rsquo;}(x_i)\\mathbb{I}(f(x_i)\\neq h(x_i)) $$ 上式中$D{t}^{\u0026rsquo;}(x_i)$可以看作一个分布，我们将其规范化$\\mathcal{D}{t}(x_i)=\\frac{\\mathcal{D}t^{\u0026rsquo;}(x_i)}{\\sum{i=1}^{|D|}\\mathcal{D}t^{\u0026rsquo;}(x_i)}$。 第t轮的样本权重可以通过第t-1轮的样本权重计算。 $$ \\begin{aligned} \\mathcal{D}{t+1}(x_i)\u0026amp;=\\mathcal{D}(x_i)e^{-f(x_i)H_t(x_i)}\\\u0026amp;=\\mathcal{D}(x_i)e^{-f(x_i)(H{t-1}(x_i)+\\alpha_t h_t(x_i))}\\ \u0026amp;=\\mathcal{D}(x_i)e^{-f(x_i)H_{t-1}(x_i)}e^{-f(x_i)\\alpha_t h_t(x_i)}\\ \u0026amp;=\\mathcal{D}(x_i)e^{-f(x_i)\\alpha_t h_t(x_i)} \\end{aligned} $$ 接下来求解学习器$h_t$权重$\\alpha_t$。损失函数$\\mathcal{L}(H_{t-1}+\\alpha h|D)$对$\\alpha$求导有： $$ \\begin{aligned} \\frac{\\partial \\mathcal{L}(H_{t-1}+\\alpha h_t|D)}{\\partial \\alpha}\u0026amp;=\\frac{\\partial(e^{-\\alpha}\\sum_{i=1}^{|D|}\\mathcal{D}t^{\u0026rsquo;}(x_i)+(e^{\\alpha}-e^{-\\alpha})\\sum{i=1}^{|D|}\\mathcal{D}{t}^{\u0026rsquo;}(x_i)\\mathbb{I}(f(x_i)\\neq h(x_i)))}{\\partial \\alpha}\\ \u0026amp;=-e^{- \\alpha}\\sum{i=1}^{|D|}\\mathcal{D}t^{\u0026rsquo;}(x_i)+(e^{\\alpha}+e^{- \\alpha})\\sum{i=1}^{|D|}\\mathcal{D}{t}^{\u0026rsquo;}(x_i)\\mathbb{I}(f(x_i)\\neq h(x_i)) \\end{aligned} $$ 令导数为0，移向得： $$ \\begin{aligned} \\frac{e^{- \\alpha}}{e^{\\alpha}+e^{- \\alpha}}\u0026amp;=\\frac{\\sum{i=1}^{|D|}\\mathcal{D}{t}^{\u0026rsquo;}(x_i)\\mathbb{I}(f(x_i) \\neq h(x_i))}{\\sum{i=1}^{|D|}\\mathcal{D}{t}^{\u0026rsquo;}(x_i)}\\ \u0026amp;=\\sum{i=1}^{|D|}\\frac{\\mathcal{D}(x_i)}{Z_t}\\mathbb{I}(f(x_i)\\neq h(x_i))\\ \u0026amp;=\\sum_{i=1}^{|D|}\\mathcal{D}t(x_i)\\mathbb{I}(f(x_i)\\neq h(x_i))\\ \u0026amp;=\\mathbb{E}{x\\sim \\mathcal{D}_t}[\\mathbb{I}f(x_i)\\neq h(x_i)]\\ \u0026amp;=\\epsilon_t \\end{aligned} $$ 求解上式可得 $$ \\alpha_t=\\frac{1}{2}\\ln(\\frac{1-\\epsilon_t}{\\epsilon_t}) $$ 以上即为参数求解步骤，算法实现如下： 输入：\n 训练集：$D={(x_1,y_1),(x_2,y_2),\\cdots,(x_m,y_m)}$ 基学习算法$\\mathcal{E}$ 训练轮数$T$  过程：\n $\\mathcal{D}_1(x)=\\frac{1}{m}$ for t=1,2,\u0026hellip;T do  $h_t=\\mathcal{E}(D,\\mathcal{D}_t)$ $\\epsilon=P_{x \\sim \\mathcal{D_t}}(h_t(x)\\neq f(x))$ if $\\epsilon_t\u0026gt;0.5$ then break $\\alpha_t=\\frac{1}{2}\\ln(\\frac{1-\\epsilon_t}{\\epsilon_t})$ 按照上面的公式从$\\mathcal{D}t$求得$\\mathcal{D}{t+1}$   end for  输出： $F(x)=sign(\\sum_{t=1}^{T}\\alpha_t h_t(x))$\n关于随机森林： Bagging和随机森林都是基学习器为决策树的算法，随机森林对基学习器进行训练时，都用自主采样法对数据集进行采样生成新的训练集并随机选择属性，这样的扰动大大提升了学习器的性能。 具体实现如下： 输入：\n 训练集：$D={(x_1,y_1),(x_2,y_2),\\cdots,(x_m,y_m)}$ 基学习算法$\\mathcal{E}$ 训练轮数$T$  过程：\n for t=1,2,\u0026hellip;,T do  从$D$采样的$\\mathcal{D}_{bs}$ 从划分属性采样得pro $h_t=\\mathcal{E}(D,\\mathcal{D}_{bs},pro)$   end for  输出：\n $H(x)=argmax_{y \\in \\mathcal{Y}\\sum_{t=1}^T\\mathbb{I}(h_t(x)=y)}$  ","date":"2022-05-04T21:37:00+02:00","image":"https://wuzhizhe7273.github.io/image/seele.png","permalink":"https://wuzhizhe7273.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BD%9C%E4%B8%9A2/","title":"机器学习作业2"},{"content":"这是测试文章1\n一级标题1 怎么没反应\n二级标题1 二级标题2 一级标题2 ","date":"2020-08-10T01:00:00+02:00","image":"https://wuzhizhe7273.github.io/image/seele.png","permalink":"https://wuzhizhe7273.github.io/p/test1/","title":"测试文章1"},{"content":"这是测试文章2\n","date":"2020-08-10T01:00:00+02:00","image":"https://wuzhizhe7273.github.io/image/seele.png","permalink":"https://wuzhizhe7273.github.io/p/test2/","title":"测试文章2"},{"content":"这是测试文章3\n","date":"2020-08-10T01:00:00+02:00","image":"https://wuzhizhe7273.github.io/image/seele.png","permalink":"https://wuzhizhe7273.github.io/p/test3/","title":"测试文章3"}]